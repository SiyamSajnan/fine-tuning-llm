{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNFewUrtUaFg"
      },
      "outputs": [],
      "source": [
        "! pip install datasets ipywidgets transformers accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "FWJYPqj-JTYq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e5OB8685hWHW"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Optional, Union, Any, Tuple\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "from scipy.stats import spearmanr\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gzip\n",
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYyZ-fo-UaFi",
        "outputId": "79877622-c048-40b3-9ac9-ff81f49fd586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "oKd63ogoJZYp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzXAVVK4UaFi"
      },
      "source": [
        "The Tokenizer method, for now the maximum length is fixed to 512..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wlya304lhWHX"
      },
      "outputs": [],
      "source": [
        "class CustomDataTokenizer:\n",
        "    def __init__(self, tokenizer, is_classification=True, max_length = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.is_classification = is_classification\n",
        "\n",
        "    def __call__(self, data: Dict) -> Dict:\n",
        "        # For Classification Task: Only Text1 is required...\n",
        "        text_columns = ['text1']\n",
        "        if not self.is_classification:\n",
        "            text_columns.append('text2')\n",
        "\n",
        "        tokens_list = []\n",
        "        for text_column in text_columns:\n",
        "            # Tokenization happens here to get in the form which is accepted in the Objective Function...\n",
        "            tokens_list.append(self.tokenizer(data[text_column], max_length=self.max_length, truncation=True))\n",
        "\n",
        "        token = {}\n",
        "        seperate_ids = []\n",
        "        for i, t in enumerate(tokens_list):\n",
        "            for key, val in t.items():\n",
        "                if i == 0:\n",
        "                    token[key] = val\n",
        "                else:\n",
        "                    token[key] += val\n",
        "                if key == 'input_ids':\n",
        "                    seperate_ids += [i] * len(val)\n",
        "\n",
        "        token['labels'] = [int(data['label']) if 'label' in data else -1]\n",
        "        token['seperate_ids'] = seperate_ids\n",
        "\n",
        "        return token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses"
      ],
      "metadata": {
        "id": "Wf43nyBfJpUi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkSrRaOJUaFj"
      },
      "source": [
        "The loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ZOw5jWY2hWHX"
      },
      "outputs": [],
      "source": [
        "def mse_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float) -> torch.Tensor:\n",
        "    if y_true.dtype != torch.long:\n",
        "        y_true = y_true.long()\n",
        "\n",
        "    batch_size = y_true.shape[0] // 2\n",
        "    y_true_pairs = y_true.view(batch_size, 2)\n",
        "    y_pred_pairs = y_pred.view(batch_size, 2, -1)\n",
        "\n",
        "    loss = 0.0\n",
        "    for i in range(batch_size):\n",
        "        y_true_pair = y_true_pairs[i]\n",
        "        y_pred_pair = y_pred_pairs[i]\n",
        "        print(y_true_pair.shape)\n",
        "        print(y_pred_pair.shape)\n",
        "        loss += torch.mean((y_true_pair - y_pred_pair) ** 2)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def default_crossentropy(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
        "    if y_true.dtype != torch.long:\n",
        "        y_true = y_true.long()\n",
        "\n",
        "    batch_size = y_true.shape[0] // 2\n",
        "    y_true_pairs = y_true.view(batch_size, 2)\n",
        "    y_pred_pairs = y_pred.view(batch_size, 2, -1)\n",
        "\n",
        "    loss = 0.0\n",
        "    for i in range(batch_size):\n",
        "        y_true_pair = y_true_pairs[i]\n",
        "        y_pred_pair = y_pred_pairs[i]\n",
        "        loss += F.cross_entropy(y_pred_pair, y_true_pair)\n",
        "\n",
        "    loss /= batch_size\n",
        "    return loss\n",
        "\n",
        "def categorical_crossentropy(y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n",
        "    return -(F.log_softmax(y_pred, dim=1) * y_true).sum(dim=1)\n",
        "\n",
        "def cosine_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float = 20.0) -> torch.Tensor:\n",
        "    y_true = y_true[::2, 0]\n",
        "    y_true = (y_true[:, None] < y_true[None, :]).float()\n",
        "    y_pred = F.normalize(y_pred, p=2, dim=1)\n",
        "    y_pred = torch.sum(y_pred[::2] * y_pred[1::2], dim=1) * tau\n",
        "    y_pred = y_pred[:, None] - y_pred[None, :]\n",
        "    y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n",
        "    zero = torch.Tensor([0]).to(y_pred.device)\n",
        "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "    return torch.logsumexp(y_pred, dim=0)\n",
        "\n",
        "def angle_loss(y_true: torch.Tensor, y_pred: torch.Tensor, tau: float = 1.0):\n",
        "    y_true = y_true[::2, 0]\n",
        "    y_true = (y_true[:, None] < y_true[None, :]).float()\n",
        "\n",
        "    y_pred_re, y_pred_im = torch.chunk(y_pred, 2, dim=1)\n",
        "    a = y_pred_re[::2]\n",
        "    b = y_pred_im[::2]\n",
        "    c = y_pred_re[1::2]\n",
        "    d = y_pred_im[1::2]\n",
        "\n",
        "    z = torch.sum(c**2 + d**2, dim=1, keepdim=True)\n",
        "    re = (a * c + b * d) / z\n",
        "    im = (b * c - a * d) / z\n",
        "\n",
        "    dz = torch.sum(a**2 + b**2, dim=1, keepdim=True)**0.5\n",
        "    dw = torch.sum(c**2 + d**2, dim=1, keepdim=True)**0.5\n",
        "    re /= (dz / dw)\n",
        "    im /= (dz / dw)\n",
        "\n",
        "    y_pred = torch.concat((re, im), dim=1)\n",
        "    y_pred = torch.abs(torch.sum(y_pred, dim=1)) * tau\n",
        "    y_pred = y_pred[:, None] - y_pred[None, :]\n",
        "    y_pred = (y_pred - (1 - y_true) * 1e12).view(-1)\n",
        "    zero = torch.Tensor([0]).to(y_pred.device)\n",
        "    y_pred = torch.concat((zero, y_pred), dim=0)\n",
        "    return torch.logsumexp(y_pred, dim=0)\n",
        "\n",
        "def in_batch_negative_loss(y_true: torch.Tensor,\n",
        "                           y_pred: torch.Tensor,\n",
        "                           tau: float = 20.0,\n",
        "                           negative_weights: float = 0.0) -> torch.Tensor:\n",
        "    device = y_true.device\n",
        "\n",
        "    def make_target_matrix(y_true: torch.Tensor):\n",
        "        idxs = torch.arange(0, y_pred.shape[0]).int().to(device)\n",
        "        y_true = y_true.int()\n",
        "        idxs_1 = idxs[None, :]\n",
        "        idxs_2 = (idxs + 1 - idxs % 2 * 2)[:, None]\n",
        "\n",
        "        idxs_1 *= y_true.T\n",
        "        idxs_1 += (y_true.T == 0).int() * -2\n",
        "\n",
        "        idxs_2 *= y_true\n",
        "        idxs_2 += (y_true == 0).int() * -1\n",
        "\n",
        "        y_true = (idxs_1 == idxs_2).float()\n",
        "        return y_true\n",
        "\n",
        "    neg_mask = make_target_matrix(y_true == 0)\n",
        "\n",
        "    y_true = make_target_matrix(y_true)\n",
        "\n",
        "    y_pred = F.normalize(y_pred, dim=1, p=2)\n",
        "    similarities = y_pred @ y_pred.T\n",
        "    similarities = similarities - torch.eye(y_pred.shape[0]).to(device) * 1e12\n",
        "    similarities = similarities * tau\n",
        "\n",
        "    if negative_weights > 0:\n",
        "        similarities += neg_mask * negative_weights\n",
        "\n",
        "    return categorical_crossentropy(y_true, similarities).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Xwuv6RUaFk"
      },
      "source": [
        "Combining the loss functions with weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "adhkDi1bhWHX"
      },
      "outputs": [],
      "source": [
        "class TotalLoss:\n",
        "    def __init__(self,\n",
        "                w1: float = 1.0,\n",
        "                w2: float = 1.0,\n",
        "                w3: float = 1.0,\n",
        "                cosine_tau: float = 20.0,\n",
        "                ibn_tau: float = 20.0,\n",
        "                angle_tau: float = 1.0):\n",
        "        self.w1 = w1\n",
        "        self.w2 = w2\n",
        "        self.w3 = w3\n",
        "        self.cosine_tau = cosine_tau\n",
        "        self.ibn_tau = ibn_tau\n",
        "        self.angle_tau = angle_tau\n",
        "\n",
        "    def __call__(self, labels: torch.Tensor, outputs: torch.Tensor) -> torch.Tensor:\n",
        "        loss = 0.\n",
        "        if (self.w1 == 0 and self.w2 == 0 and self.w3 == 0):\n",
        "            loss += default_crossentropy(labels, outputs)\n",
        "        if self.w1 > 0:\n",
        "            loss += self.w1 * mse_loss(labels, outputs, self.cosine_tau)\n",
        "        if self.w2 > 0:\n",
        "            loss += self.w2 * in_batch_negative_loss(labels, outputs, self.ibn_tau)\n",
        "        if self.w3 > 0:\n",
        "            loss += self.w3 * angle_loss(labels, outputs, self.angle_tau)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooler"
      ],
      "metadata": {
        "id": "bXi45wR2KFFv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz8a3YGkUaFk"
      },
      "source": [
        "The different Pooling methods, using CLS for now, and Padding Strategy 'Left' for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xyFWXUQXhWHY"
      },
      "outputs": [],
      "source": [
        "class Pooler:\n",
        "    def __init__(self,\n",
        "                model,\n",
        "                # ['cls', 'cls_avg', 'last', 'avg', 'max', 'all', 'specific token index']\n",
        "                pooling_strategy: Optional[Union[int, str]] = 'cls',\n",
        "                padding_strategy: Optional[str] = 'left'):\n",
        "        self.model = model\n",
        "        self.pooling_strategy = pooling_strategy\n",
        "        self.padding_strategy = padding_strategy\n",
        "\n",
        "    def __call__(self, inputs) -> Any:\n",
        "        if self.pooling_strategy == 'last':\n",
        "            batch_size = inputs['input_ids'].shape[0]\n",
        "            if self.padding_strategy == 'left':\n",
        "                sequence_lengths = -1\n",
        "            else:\n",
        "                sequence_lengths = inputs[\"attention_mask\"].sum(dim=1) - 1\n",
        "\n",
        "        outputs = self.model(**inputs).last_hidden_state\n",
        "        if self.pooling_strategy == 'cls':\n",
        "            outputs = outputs[:, 0]\n",
        "        elif self.pooling_strategy == 'cls_avg':\n",
        "            outputs = (outputs[:, 0] + torch.mean(outputs, dim=1)) / 2.0\n",
        "        elif self.pooling_strategy == 'last':\n",
        "            outputs = outputs[torch.arange(batch_size, device=outputs.device), sequence_lengths]\n",
        "        elif self.pooling_strategy == 'avg':\n",
        "            outputs = torch.sum(\n",
        "                outputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"])\n",
        "        elif self.pooling_strategy == 'max':\n",
        "            outputs, _ = torch.max(outputs * inputs[\"attention_mask\"][:, :, None], dim=1)\n",
        "        elif self.pooling_strategy == 'all':\n",
        "            return outputs\n",
        "        elif isinstance(self.pooling_strategy, int) or self.pooling_strategy.isnumeric():\n",
        "            return outputs[:, int(self.pooling_strategy)]\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer"
      ],
      "metadata": {
        "id": "yVDqc2HoKXpU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA8qbPXoUaFl"
      },
      "source": [
        "The custom trainer method which extends the Trainer method of Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0RpRE7z8hWHY"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, pooler: Pooler, loss_kwargs: Optional[Dict] = None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.pooler = pooler\n",
        "        if loss_kwargs is None:\n",
        "            loss_kwargs = {}\n",
        "        self.loss_fct = TotalLoss(**loss_kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\", None)\n",
        "        outputs = self.pooler(inputs)\n",
        "        loss = self.loss_fct(labels, outputs)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator"
      ],
      "metadata": {
        "id": "yDR2QBJiKnBf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx8iSK42UaFl"
      },
      "source": [
        "The custom data collator which works with the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xar9ji1NhWHY"
      },
      "outputs": [],
      "source": [
        "class CustomDataCollator:\n",
        "    tokenizer = None\n",
        "    padding = 'longest'\n",
        "    max_length: Optional[int] = 512\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __init__(self, tokenizer_base):\n",
        "        self.tokenizer = tokenizer_base\n",
        "\n",
        "    def __call__(self, features: List[Dict], return_tensors: str = \"pt\") -> Dict[str, torch.Tensor]:\n",
        "        if return_tensors is None:\n",
        "            return_tensors = self.return_tensors\n",
        "        has_token_type_ids = \"token_type_ids\" in features[0]\n",
        "\n",
        "        new_features = []\n",
        "        for feature in features:\n",
        "            seperate_ids = feature['seperate_ids']\n",
        "            input_ids = feature['input_ids']\n",
        "            attention_mask = feature['attention_mask']\n",
        "            assert len(seperate_ids) == len(input_ids) == len(attention_mask)\n",
        "\n",
        "            has_token_type_ids = False\n",
        "            if \"token_type_ids\" in feature:\n",
        "                has_token_type_ids = True\n",
        "                token_type_ids = feature['token_type_ids']\n",
        "                assert len(token_type_ids) == len(input_ids)\n",
        "\n",
        "            max_seperate_id = max(seperate_ids)\n",
        "            prev_start_idx = 0\n",
        "            for seperate_id in range(1, max_seperate_id + 1):\n",
        "                start_idx = seperate_ids.index(seperate_id)\n",
        "\n",
        "                new_feature = {}\n",
        "                new_feature['input_ids'] = input_ids[prev_start_idx:start_idx]\n",
        "                new_feature['attention_mask'] = attention_mask[prev_start_idx:start_idx]\n",
        "                if has_token_type_ids:\n",
        "                    new_feature['token_type_ids'] = token_type_ids[prev_start_idx:start_idx]\n",
        "                new_feature['labels'] = feature['labels']\n",
        "                new_features.append(new_feature)\n",
        "                prev_start_idx = start_idx\n",
        "\n",
        "            new_feature = {}\n",
        "            new_feature['input_ids'] = input_ids[prev_start_idx:]\n",
        "            new_feature['attention_mask'] = attention_mask[prev_start_idx:]\n",
        "            if has_token_type_ids:\n",
        "                new_feature['token_type_ids'] = token_type_ids[prev_start_idx:]\n",
        "            new_feature['labels'] = feature['labels']\n",
        "            new_features.append(new_feature)\n",
        "\n",
        "        del features\n",
        "        features = self.tokenizer.pad(\n",
        "            {'input_ids': [feature['input_ids'] for feature in new_features]},\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=return_tensors,\n",
        "        )\n",
        "        features['attention_mask'] = self.tokenizer.pad(\n",
        "            {'input_ids': [feature['attention_mask'] for feature in new_features]},\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=return_tensors,\n",
        "        )['input_ids']\n",
        "        if has_token_type_ids:\n",
        "            features['token_type_ids'] = self.tokenizer.pad(\n",
        "                {'input_ids': [feature['token_type_ids'] for feature in new_features]},\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=return_tensors,\n",
        "            )['input_ids']\n",
        "        features['labels'] = torch.Tensor([feature['labels'] for feature in new_features])\n",
        "\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fit"
      ],
      "metadata": {
        "id": "kDsjQhxkKq1Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajV4GQVDUaFm"
      },
      "source": [
        "The fit method which starts the training process, for now a lot of arguments have provided with default value..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_41sJ58WhWHY"
      },
      "outputs": [],
      "source": [
        "def fit(train_ds,\n",
        "        model_base,\n",
        "        tokenizer_base,\n",
        "        batch_size: int = 32,\n",
        "        output_dir: Optional[str] = 'chk/new_c',\n",
        "        epochs: int = 5,\n",
        "        learning_rate: float = 1e-5,\n",
        "        warmup_steps: int = 1000,\n",
        "        logging_steps: int = 10,\n",
        "        eval_steps: Optional[int] = None,\n",
        "        save_steps: int = 100,\n",
        "        save_strategy: str = 'steps',\n",
        "        save_total_limit: int = 10,\n",
        "        gradient_accumulation_steps: int = 1,\n",
        "        fp16: Optional[bool] = None,\n",
        "        argument_kwargs: Optional[Dict] = None,\n",
        "        trainer_kwargs: Optional[Dict] = None,\n",
        "        loss_kwargs: Optional[Dict] = None):\n",
        "\n",
        "    if argument_kwargs is None:\n",
        "        argument_kwargs = {}\n",
        "    if trainer_kwargs is None:\n",
        "        trainer_kwargs = {}\n",
        "    callbacks = None\n",
        "\n",
        "    pooler = Pooler(model_base)\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        pooler=pooler,\n",
        "        model=model_base,\n",
        "        train_dataset=train_ds,\n",
        "        loss_kwargs=loss_kwargs,\n",
        "        tokenizer=tokenizer_base,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            warmup_steps=warmup_steps,\n",
        "            num_train_epochs=epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            fp16=fp16,\n",
        "            logging_steps=logging_steps,\n",
        "            save_strategy=save_strategy,\n",
        "            eval_steps=eval_steps,\n",
        "            save_steps=save_steps,\n",
        "            output_dir=output_dir,\n",
        "            save_total_limit=save_total_limit,\n",
        "            load_best_model_at_end=False,\n",
        "            ddp_find_unused_parameters=None,\n",
        "            label_names=['labels', 'seperate_ids', 'extra'],\n",
        "            **argument_kwargs,\n",
        "        ),\n",
        "        callbacks=callbacks,\n",
        "        data_collator=CustomDataCollator(\n",
        "            tokenizer_base\n",
        "        ),\n",
        "        **trainer_kwargs\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model_base, tokenizer_base, pooler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "OpiS9-RxKuun"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CovZGEUAUaFm"
      },
      "source": [
        "The encode method to generate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SASA4fFghWHZ"
      },
      "outputs": [],
      "source": [
        "def encode(inputs: Union[List[str], Tuple[str], List[Dict], str],\n",
        "            model,\n",
        "            pooler,\n",
        "            tokenizer,\n",
        "            max_length: Optional[int] = 512,\n",
        "            to_numpy: bool = True,\n",
        "            device: Optional[Any] = 'cuda:0'):\n",
        "        if device is None:\n",
        "            device = 'cpu'\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        tokens = tokenizer(\n",
        "            inputs,\n",
        "            padding='longest',\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            return_tensors='pt')\n",
        "        tokens.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = pooler(tokens)\n",
        "        if to_numpy:\n",
        "            return output.float().detach().cpu().numpy()\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution"
      ],
      "metadata": {
        "id": "npUyhInUK6qZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Import"
      ],
      "metadata": {
        "id": "n_yqmnGuKxOH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfBmu_0-6wGZ"
      },
      "source": [
        "SentEval Datasets Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b0x3s3P_6zAe"
      },
      "outputs": [],
      "source": [
        "def get_senteval_binary_data(dataset):\n",
        "    match dataset:\n",
        "        case 'CR':\n",
        "            pos_file = open(\"../Data/custrev.pos\", \"r\")\n",
        "            neg_file = open(\"../Data/custrev.neg\", \"r\")\n",
        "        case 'MPQA':\n",
        "            pos_file = open(\"../Data/mpqa.pos\", \"r\")\n",
        "            neg_file = open(\"../Data/mpqa.neg\", \"r\")\n",
        "        case 'MR':\n",
        "            pos_file = open(\"../Data/rt-polarity.pos\", \"r\")\n",
        "            neg_file = open(\"../Data/rt-polarity.neg\", \"r\")\n",
        "        case 'SUBJ':\n",
        "            pos_file = open(\"../Data/subj.objective\", \"r\")\n",
        "            neg_file = open(\"../Data/subj.subjective\", \"r\")\n",
        "\n",
        "    df_pos = pd.DataFrame()\n",
        "    pos_content = pos_file.readlines()\n",
        "    pos_file.close()\n",
        "    df_pos['sentence'] = pos_content\n",
        "    labels = np.ones(len(pos_content))\n",
        "    df_pos['label'] = labels.astype('int')\n",
        "\n",
        "    df_neg = pd.DataFrame()\n",
        "    neg_content = neg_file.readlines()\n",
        "    neg_file.close()\n",
        "    df_neg['sentence'] = neg_content\n",
        "    labels = np.zeros(len(neg_content))\n",
        "    df_neg['label'] = labels.astype('int')\n",
        "\n",
        "    df = pd.concat([df_pos, df_neg], axis=0, ignore_index=True)\n",
        "    df['sentence'] = df['sentence'].str.replace('\\n', '')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kG8RjUks61wY"
      },
      "outputs": [],
      "source": [
        "# SST -> We use the Binary classification...\n",
        "def get_sst_data():\n",
        "    train_file = open(\"../Data/sentiment-train\", \"r\")\n",
        "    train = train_file.readlines()\n",
        "    train_file.close()\n",
        "\n",
        "    sentence_train = []\n",
        "    label_train = []\n",
        "    for sentence in train:\n",
        "        sentence = sentence.strip()\n",
        "        label = int(sentence[len(sentence) - 1])\n",
        "        sentence = sentence[:-1].strip()\n",
        "        sentence_train.append(sentence)\n",
        "        label_train.append(label)\n",
        "\n",
        "    df_train = pd.DataFrame({'sentence': sentence_train, 'label': label_train})\n",
        "\n",
        "    test_file = open(\"../Data/sentiment-test\", \"r\")\n",
        "    test = test_file.readlines()\n",
        "    test_file.close()\n",
        "\n",
        "    sentence_test = []\n",
        "    label_test = []\n",
        "    for sentence in test:\n",
        "        sentence = sentence.strip()\n",
        "        label = int(sentence[len(sentence) - 1])\n",
        "        sentence = sentence[:-1].strip()\n",
        "        sentence_test.append(sentence)\n",
        "        label_test.append(label)\n",
        "\n",
        "    df_test = pd.DataFrame({'sentence': sentence_test, 'label': label_test})\n",
        "    df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8Y3rYibz620e"
      },
      "outputs": [],
      "source": [
        "def get_senteval_dataset(name):\n",
        "    match name:\n",
        "        case 'CR':\n",
        "            return get_senteval_binary_data('CR')\n",
        "        case 'MPQA':\n",
        "            return get_senteval_binary_data('MPQA')\n",
        "        case 'MR':\n",
        "            return get_senteval_binary_data('MR')\n",
        "        case 'SST':\n",
        "            return get_sst_data()\n",
        "        case 'SUBJ':\n",
        "            return get_senteval_binary_data('SUBJ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ysILLEOe7CJr"
      },
      "outputs": [],
      "source": [
        "senteval_datasets = ['CR', 'MPQA', 'MR', 'SUBJ']#, 'SST']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi9izGqz7rC7"
      },
      "source": [
        "STS Datasets Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SNHC4DNl7v7T"
      },
      "outputs": [],
      "source": [
        "def get_sts_dataset(dataset_name):\n",
        "    match dataset_name:\n",
        "        case 'STS-B':\n",
        "            dataset = load_dataset('mteb/stsbenchmark-sts', split='test')\n",
        "        case 'STS12':\n",
        "            dataset = load_dataset('mteb/sts12-sts', split='test')\n",
        "        case 'STS13':\n",
        "            dataset = load_dataset('mteb/sts13-sts', split='test')\n",
        "        case 'STS14':\n",
        "            dataset = load_dataset('mteb/sts14-sts', split='test')\n",
        "        case 'STS15':\n",
        "            dataset = load_dataset('mteb/sts15-sts', split='test')\n",
        "        case 'STS16':\n",
        "            dataset = load_dataset('mteb/sts16-sts', split='test')\n",
        "        case 'SICK-R':\n",
        "            dataset = load_dataset('mteb/sickr-sts', split='test')\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zMWL7F6P8EG6"
      },
      "outputs": [],
      "source": [
        "sts_datasets = ['STS-B']#['STS-B', 'STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'SICK-R']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlAwf-fRaFIL"
      },
      "source": [
        "## Objective Function Combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the 3 provided objective functions and their 7 possible combinations for now"
      ],
      "metadata": {
        "id": "lGj_X7A8LAwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "s9vafqNKa_-t"
      },
      "outputs": [],
      "source": [
        "def get_objective_function_weights(combi):\n",
        "    match combi:\n",
        "        case 'Default Cross-Entropy': return (0, 0, 0)\n",
        "        case 'Cosine': return (1, 0, 0)\n",
        "        case 'In-Batch Negatives': return (0, 1, 0)\n",
        "        case 'Angle': return (0, 0, 1)\n",
        "        case 'Cosine + In-Batch Negatives': return (1, 1, 0)\n",
        "        case 'Cosine + Angle': return (1, 0, 1)\n",
        "        case 'In-Batch Negatives + Angle': return (0, 1, 1)\n",
        "        case 'Cosine + In-Batch Negatives + Angle': return (1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "hAEJCLCEaQ0Q"
      },
      "outputs": [],
      "source": [
        "objective_functions = [\n",
        "    'Default Cross-Entropy',\n",
        "    'Cosine',\n",
        "    'In-Batch Negatives',\n",
        "    'Angle',\n",
        "    'Cosine + In-Batch Negatives',\n",
        "    'Cosine + Angle',\n",
        "    'In-Batch Negatives + Angle',\n",
        "    'Cosine + In-Batch Negatives + Angle'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Models"
      ],
      "metadata": {
        "id": "xszL7wkILGFh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGBEfvPVhWHZ"
      },
      "source": [
        "Base Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UeMjLiE--JRS"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    'bert-base-uncased',\n",
        "    'bert-base-cased',\n",
        "    'bert-large-uncased',\n",
        "    'bert-large-cased'\n",
        "    'FacebookAI/roberta-base',\n",
        "    'sentence-transformers/all-mpnet-base-v2',\n",
        "    'princeton-nlp/sup-simcse-roberta-large'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Driver Functions"
      ],
      "metadata": {
        "id": "KciCira7LJpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SentEval"
      ],
      "metadata": {
        "id": "w8VR7dhZLh55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FtecQezwb5Ui"
      },
      "outputs": [],
      "source": [
        "def driver_senteval():\n",
        "    results_matrix = []\n",
        "\n",
        "    for model in models:\n",
        "        results_obj_matrix = []\n",
        "        for objective in objective_functions:\n",
        "            # Objective Functions Preparation...\n",
        "            w1_combi, w2_combi, w3_combi = get_objective_function_weights(objective)\n",
        "            results_obj_ds_matrix = []\n",
        "\n",
        "            for dataset in senteval_datasets:\n",
        "                # Model Preparation...\n",
        "                tokenizer_base = AutoTokenizer.from_pretrained(model)\n",
        "                model_base = AutoModel.from_pretrained(model)\n",
        "\n",
        "                # Dataset Preparation...\n",
        "                df = get_senteval_dataset(dataset)\n",
        "                ds = Dataset.from_pandas(df)\n",
        "                ds = ds.rename_column('sentence', 'text1')\n",
        "                if dataset == 'MR':\n",
        "                    ds = concatenate_datasets([ds.select(range(0, 231)), ds.select(range(233, 7463))])\n",
        "\n",
        "                split_ds = ds.train_test_split(test_size=0.3, seed=42)\n",
        "                ds_train = split_ds['train']\n",
        "                ds_test = split_ds['test']\n",
        "\n",
        "                # Tokenization...\n",
        "                train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base), num_proc=8)\n",
        "\n",
        "                # Model Training...\n",
        "                model_new, tokenizer_new, pooler_new = fit(\n",
        "                    train_ds=train_ds,\n",
        "                    model_base=model_base,\n",
        "                    tokenizer_base=tokenizer_base,\n",
        "                    output_dir='chk/c',\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    learning_rate=2e-5,\n",
        "                    save_steps=0,\n",
        "                    eval_steps=100,\n",
        "                    warmup_steps=0,\n",
        "                    gradient_accumulation_steps=1,\n",
        "                    loss_kwargs={\n",
        "                        'w1': w1_combi,\n",
        "                        'w2': w2_combi,\n",
        "                        'w3': w3_combi,\n",
        "                        'cosine_tau': 20,\n",
        "                        'ibn_tau': 20,\n",
        "                        'angle_tau': 1.0\n",
        "                    },\n",
        "                    fp16=True,\n",
        "                    logging_steps=1000\n",
        "                )\n",
        "\n",
        "                # Embedding Generation for Train and Test sets... Doing line-by-line embeddings for now...\n",
        "                emb_train = []\n",
        "                for sentence in ds_train['text1']:\n",
        "                    emb_train.append(encode(sentence, model_new, pooler_new, tokenizer_new)[0])\n",
        "\n",
        "                emb_test = []\n",
        "                for sentence in ds_test['text1']:\n",
        "                    emb_test.append(encode(sentence, model_new, pooler_new, tokenizer_new)[0])\n",
        "\n",
        "                # Conversion into Numpy Array...\n",
        "                emb_train = np.array(emb_train)\n",
        "                emb_test = np.array(emb_test)\n",
        "\n",
        "                # Classification...\n",
        "                lr = LogisticRegression(max_iter=10000)\n",
        "                lr.fit(emb_train, ds_train['label'])\n",
        "                accuracy_score = lr.score(emb_test, ds_test['label'])\n",
        "                print('Model: ', model)\n",
        "                print('Loss: ', objective)\n",
        "                print('Dataset: ', dataset)\n",
        "                print('Accuracy: ', accuracy_score)\n",
        "                results_obj_ds_matrix.append(accuracy_score)\n",
        "            results_obj_matrix.append(results_obj_ds_matrix)\n",
        "        results_matrix.append(results_obj_matrix)\n",
        "    return results_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STS"
      ],
      "metadata": {
        "id": "Aoik9A2OLk5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kj-Y2cnGhWHc"
      },
      "outputs": [],
      "source": [
        "def calculate_cosine_similarity(sentence1_vec, sentence2_vec):\n",
        "    cosine_similarity = np.dot(sentence1_vec, sentence2_vec) / (np.linalg.norm(sentence1_vec) * np.linalg.norm(sentence2_vec))\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XWhPIBI7AAWo"
      },
      "outputs": [],
      "source": [
        "def calculate_Spearman_rank_correlation_coefficient(scores, scores_actual):\n",
        "    sc, _ = spearmanr(scores, scores_actual)\n",
        "    return sc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4dMgwpWqAAWo"
      },
      "outputs": [],
      "source": [
        "def driver_sts():\n",
        "    results_matrix = []\n",
        "\n",
        "    for model in models:\n",
        "        results_obj_matrix = []\n",
        "        for objective in objective_functions:\n",
        "            # Objective Functions Preparation...\n",
        "            w1_combi, w2_combi, w3_combi = get_objective_function_weights(objective)\n",
        "            results_obj_ds_matrix = []\n",
        "\n",
        "            for dataset in sts_datasets:\n",
        "                # Model Preparation...\n",
        "                tokenizer_base = AutoTokenizer.from_pretrained(model)\n",
        "                model_base = AutoModel.from_pretrained(model)\n",
        "\n",
        "                # Dataset Preparation...\n",
        "                ds = get_sts_dataset(dataset)\n",
        "                ds = ds.rename_column('sentence1', 'text1')\n",
        "                ds = ds.rename_column('sentence2', 'text2')\n",
        "                ds = ds.rename_column('score', 'label')\n",
        "\n",
        "                split_ds = ds.train_test_split(test_size=0.3, seed=42)\n",
        "                ds_train = split_ds['train']\n",
        "                ds_test = split_ds['test']\n",
        "\n",
        "                # Tokenization of train dataset...\n",
        "                train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base, is_classification=False), num_proc=8)\n",
        "\n",
        "                # Model Training...\n",
        "                model_new, tokenizer_new, pooler_new = fit(\n",
        "                    train_ds=train_ds,\n",
        "                    model_base=model_base,\n",
        "                    tokenizer_base=tokenizer_base,\n",
        "                    output_dir='chk/c',\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    learning_rate=2e-5,\n",
        "                    save_steps=0,\n",
        "                    eval_steps=100,\n",
        "                    warmup_steps=0,\n",
        "                    gradient_accumulation_steps=1,\n",
        "                    loss_kwargs={\n",
        "                        'w1': w1_combi,\n",
        "                        'w2': w2_combi,\n",
        "                        'w3': w3_combi,\n",
        "                        'cosine_tau': 20,\n",
        "                        'ibn_tau': 20,\n",
        "                        'angle_tau': 1.0\n",
        "                    },\n",
        "                    fp16=True,\n",
        "                    logging_steps=1000\n",
        "                )\n",
        "\n",
        "                # Generating embeddings of STS dataset using the newly trained model...\n",
        "                emb_sentence_1 = encode(ds_test['text1'], model_new, pooler_new, tokenizer_new) # generating embeddings for test set sentence 1\n",
        "                emb_sentence_2 = encode(ds_test['text2'], model_new, pooler_new, tokenizer_new) # generating embeddings for test set sentence 2\n",
        "\n",
        "                # Calculating Spearman for AnglE...\n",
        "                cos_score = []\n",
        "                for i in range(emb_sentence_1.shape[0]):\n",
        "                    cos_score.append(calculate_cosine_similarity(emb_sentence_1[i], emb_sentence_2[i]))\n",
        "\n",
        "                spearman = calculate_Spearman_rank_correlation_coefficient(cos_score, ds_test['label'])\n",
        "                results_obj_ds_matrix.append(spearman)\n",
        "                print('Model: ', model)\n",
        "                print('Loss: ', objective)\n",
        "                print('Dataset: ', dataset)\n",
        "                print('Spearman: ', spearman)\n",
        "            results_obj_matrix.append(results_obj_ds_matrix)\n",
        "        results_matrix.append(results_obj_matrix)\n",
        "    return results_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running"
      ],
      "metadata": {
        "id": "YOkPXS5_LXM5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGWYzer1AAWo"
      },
      "outputs": [],
      "source": [
        "results_matrix_sts = driver_sts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2T82nNvfW6T"
      },
      "outputs": [],
      "source": [
        "results_matrix_senteval = driver_senteval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Results"
      ],
      "metadata": {
        "id": "6m5rbe2hLZeo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdDsIykFAAWt"
      },
      "outputs": [],
      "source": [
        "with open('../Results/BERT_SentEval_Results.npy', 'wb') as f:\n",
        "    np.save(f, results_matrix_senteval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAJd7esKAAWu"
      },
      "outputs": [],
      "source": [
        "with open('../Results/BERT_STS_Results.npy', 'wb') as f:\n",
        "    np.save(f, results_matrix_sts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W8o9ZmMUaFn"
      },
      "source": [
        "# NLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYV5wydqUaFn"
      },
      "outputs": [],
      "source": [
        "def load_all_nli(exclude_neutral=True):\n",
        "    label_mapping = {\n",
        "        'entailment': 1,  # '0' (entailment)\n",
        "        'neutral': 1,\n",
        "        'contradiction': 0   # '2' (contradiction)\n",
        "    }\n",
        "    data = []\n",
        "    with gzip.open('AllNLI.tsv.gz', 'rt', encoding='utf8') as fIn:\n",
        "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
        "        for row in reader:\n",
        "            if row['split'] == 'train' and row['label'] != 'neutral':\n",
        "                if exclude_neutral and row['label'] == 'neutral':\n",
        "                    continue\n",
        "                sent1 = row['sentence1'].strip()\n",
        "                sent2 = row['sentence2'].strip()\n",
        "                data.append({'text1': sent1, 'text2': sent2, 'label': label_mapping[row['label']]})\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwC1copcUaFn"
      },
      "outputs": [],
      "source": [
        "def preprocess_nli():\n",
        "    train_data = load_all_nli()\n",
        "    nli_dataset = {}\n",
        "    train_ds = Dataset.from_list(train_data)\n",
        "    nli_dataset['train'] = train_ds\n",
        "    nli_dataset = DatasetDict(nli_dataset)\n",
        "    ds_train = nli_dataset['train']\n",
        "    return ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-k7PuBPhWHb"
      },
      "source": [
        "Training with AnglE losses..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4q85vBUkUaFn"
      },
      "outputs": [],
      "source": [
        "def train_nli(ds_train, model_base, tokenizer_base):\n",
        "    train_ds = ds_train.shuffle().map(CustomDataTokenizer(tokenizer_base), num_proc=8)\n",
        "    model_new, tokenizer_new, pooler_new = fit(\n",
        "        train_ds=train_ds,\n",
        "        model_base=model_base,\n",
        "        tokenizer_base=tokenizer_base,\n",
        "        output_dir='chk/c',\n",
        "        batch_size=32,\n",
        "        epochs=5,\n",
        "        learning_rate=2e-5,\n",
        "        save_steps=0,\n",
        "        eval_steps=100,\n",
        "        warmup_steps=0,\n",
        "        gradient_accumulation_steps=1,\n",
        "        loss_kwargs={\n",
        "            'w1': 1.0,\n",
        "            'w2': 1.0,\n",
        "            'w3': 1.0\n",
        "            'cosine_tau': 20,\n",
        "            'ibn_tau': 20,\n",
        "            'angle_tau': 1.0\n",
        "        },\n",
        "        fp16=True,\n",
        "        logging_steps=1000\n",
        "    )\n",
        "    return model_new, tokenizer_new, pooler_new"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "FWJYPqj-JTYq",
        "oKd63ogoJZYp",
        "Wf43nyBfJpUi",
        "bXi45wR2KFFv",
        "yVDqc2HoKXpU",
        "yDR2QBJiKnBf",
        "kDsjQhxkKq1Y",
        "OpiS9-RxKuun",
        "npUyhInUK6qZ",
        "n_yqmnGuKxOH",
        "LlAwf-fRaFIL",
        "xszL7wkILGFh",
        "w8VR7dhZLh55",
        "Aoik9A2OLk5R",
        "YOkPXS5_LXM5",
        "6m5rbe2hLZeo",
        "-W8o9ZmMUaFn"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}